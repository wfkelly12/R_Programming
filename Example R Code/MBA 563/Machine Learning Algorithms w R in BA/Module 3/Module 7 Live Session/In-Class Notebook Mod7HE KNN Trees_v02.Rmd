************************************************************************
# Title: In-Class Notebook Mod7HE Knn Decision trees
Course: MBA563
Term: Summer 2021
Mooc or HE?: HE
Module: 07
Author: Jessen Hobson
************************************************************************

*********KNN & Decision Trees**************************************************
 In Class
*******************************************************************************
# Initial loading of data, packages, and functions
```{r}
# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
  true_positive <- cf_table[4]
  true_negative <- cf_table[1]
  false_positive <- cf_table[2]
  false_negative <- cf_table[3]
  accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
  sensitivity_recall <- true_positive / (true_positive + false_negative) 
  specificity_selectivity <- true_negative / (true_negative + false_positive)
  precision <- true_positive / (true_positive + false_positive) 
  neg_pred_value <- true_negative/(true_negative + false_negative)
  print(cf_table)
  my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
                  sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
                  sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
                  sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
                  sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy), 
                  sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
                  sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
                  sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
                  sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
  )
  return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds(r"(C:\Users\jlhobson\Box\(Focus Area 4) Business Analytics\3. Data Toolkit (Guymon Hobson)\HE SU 2021\Module 7\mod6HE_logit.rds)")
```


































*********KNN*********************************************************
# **1.0** 
# Preprocess data for knn
```{r}
# Not for the model
knn1 <- df %>% ungroup() %>% 
  select(store, week, region, high_med_rev, high_med_units, high_med_gpm)

# make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_gp = factor(if_else(high_med_gp==1, 'high', 'low'), levels=c('low', 'high'))) 
knn2 <- knn2 %>% ungroup() %>% 
  select(high_med_gp, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)

# Check that "positive" is last for the `my_confusion_matrix` to work 
contrasts(knn2$high_med_gp)
```

















# Partition the data
```{r}
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_gp, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]

# Separate the target variable from the training and testing data 
X_train <- data_train %>% select(-high_med_gp)
X_test <-  data_test %>% select(-high_med_gp) 
y_train <- data_train$high_med_gp
y_test <- data_test$high_med_gp
```



















# Features must be standardized so use z-score standardization
```{r}
X_train <- scale(X_train)
X_test <- scale(X_test)
```





















# Run the model
```{r}
#install.packages('class') #don't install twice
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
```

*Go look at questions in PowerPoint.*





















# Confusion matrix - checking accuracy
```{r}
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
```
























# Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$knn <- knn_prediction

# Create a variable that shows if the prediction was correct
data_test <- data_test %>% 
  mutate(correct_knn = if_else(knn == high_med_gp, 'correct', 'WRONG!'))

# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)

# For viewing in class
full_test_knn <- full_test_knn %>% 
  select(store, week, high_med_gp, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
```


















**********DECISION TREES****************************************************
# **2.0**
# Preprocess data
```{r}
# Not for the model
tree1 <- df %>% ungroup() %>% 
  select(store, week, high_med_rev, high_med_units, high_med_gpm)

# make the target feature and `region` a factor
tree2 <- df %>% mutate(high_med_gp = factor(if_else(high_med_gp==1, 'high', 'low'), levels=c('low', 'high')),
                       region = factor(region)) 
tree2 <- tree2 %>% ungroup() %>% 
  select(high_med_gp, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Check that "positive" is last for `my_confusion_matrix()` to work 
contrasts(tree2$high_med_gp)
```















# Use the `caret` package to split the data, 75% training and 25% testing
```{r}
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_gp, p=.75, list=FALSE)
data_train <- tree2[partition, ]
data_test <- tree2[-partition, ]
```

























# Use the `rpart()` function from the `rpart` package to train the model
```{r}
#install.packages('rpart') #(don't install twice)
#install.packages('rpart.plot') #(don't install twice)
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_gp ~ ., data_train)
```

























# Use the trained model to predict whether `high_med_gp` is high or low
```{r}
predict_tree <- predict(model_tree, data_test, type='class') #`type='class'` keeps this a factor 
```





















# Use the confusion matrix code above to examine the accuracy of this model
```{r}
table1 <- table(predict_tree, data_test$high_med_gp)
my_confusion_matrix(table1)
```





























# Using the `plot()` function draw a labeled picture of the tree model.
```{r}
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'gray', nn=TRUE)
```


























# Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$tree <- predict_tree

# Create a variable that shows if the prediction was correct
data_test <- data_test %>% 
  mutate(correct_tree = if_else(tree == high_med_gp, 'correct', 'WRONG!'))

# Add back the original data
temp1 <- tree1[-partition, ]
full_test_tree <- bind_cols(temp1, data_test)

# For viewing in class
full_test_tree <- full_test_tree %>% 
  select(store, week, high_med_gp, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)
```
































**3.0**
# Put both predictions together
```{r}
full_test <- bind_cols(full_test_knn %>% select(store, week, high_med_gp, knn, correct_knn), 
                       full_test_tree %>% select(-store, -week, -high_med_gp))
slice_sample(full_test, n=10)
```
