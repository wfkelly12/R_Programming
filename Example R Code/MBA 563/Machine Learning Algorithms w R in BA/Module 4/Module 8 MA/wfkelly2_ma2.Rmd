---
title: "wfkelly2_ma2 - Credit Card Default Payment"
author: "William Kelly"
date: "7/23/2021"
output: html_document
---

# Data Sourced
Machine Learning Repository of University of California, Irvine 
https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients

# Description of Data
This Dataset has 300001 rows and 24 columns.
The Data can be used for estimating the probability of default payment by credit card client.

# Data Dictionary
LIMIT_BAL: Amount of the given credit
SEX: Gender (1 = male, 2 = female)
EDUCTION: Education (1 =   graduate school; 2 = university; 3 = high school; 4 = others)
MARRIAGE: Marital status (1 = married; 2 = single; 3 = others) 
AGE: Age (year)
PAY_0: History of past payment. The repayment status in September, 2005*
PAY_2: History of past payment. The repayment status in August, 2005*
PAY_3: History of past payment. The repayment status in July, 2005*
PAY_4: History of past payment. The repayment status in June, 2005*
PAY_5: History of past payment. The repayment status in May, 2005*
PAY_6: History of past payment. The repayment status in April, 2005*
BILL_AMT1: Amount of bill   statement in September, 2005 (NT dollar)
BILL_AMT2: Amount of bill   statement in August, 2005 (NT dollar)
BILL_AMT3: Amount of bill   statement in July, 2005 (NT dollar)
BILL_AMT4: Amount of bill   statement in June, 2005 (NT dollar)
BILL_AMT5: Amount of bill   statement in May, 2005 (NT dollar)
BILL_AMT6: Amount of bill   statement in April, 2005 (NT dollar)
PAY_AMT1: Amount of   previous payment. Paid in September, 2005 (NT dollar)
PAY_AMT2: Amount of   previous payment. Paid in August, 2005 (NT dollar)
PAY_AMT3: Amount of   previous payment. Paid in July, 2005 (NT dollar)
PAY_AMT4: Amount of   previous payment. Paid in June, 2005 (NT dollar)
PAY_AMT5: Amount of   previous payment. Paid in May, 2005 (NT dollar)
PAY_AMT6: Amount of   previous payment. Paid in April, 2005 (NT dollar)
Default Payment Next Month: Probability of Default. (1: Yes, 0: No)


# Load Libraries
```{r}
library(tidyverse)
library(ggplot2)
library(corrplot)
```

# Import 'Default Credit Card Clients' Data Set
```{r}
df <- read.csv("default of credit card clients.csv")
```

# Call Confusion Matrix Function
```{r}
my_confusion_matrix <- function(cf_table) {
  true_positive <- cf_table[4]
  true_negative <- cf_table[1]
  false_positive <- cf_table[2]
  false_negative <- cf_table[3]
  accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
  sensitivity_recall <- true_positive / (true_positive + false_negative) 
  specificity_selectivity <- true_negative / (true_negative + false_positive)
  precision <- true_positive / (true_positive + false_positive) 
  neg_pred_value <- true_negative/(true_negative + false_negative)
  print(cf_table)
  my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
                  sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
                  sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
                  sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
                  sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy), 
                  sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
                  sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),                   
                  sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
                  sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
  )
  return(my_list)
}
```

# Assign headers one from first row and delete first row
```{r}
colnames(df) <- as.character(unlist(df[1,]))
df = df[-1, ]
head(df)
```

# Rename Target Variable (Y-var) "Default Payment Next Month" to "Default_Payment"
```{r}
colnames(df)[colnames(df)=="default payment next month"] <- "default_payment"
head(df)
```

# Change Data Type from Character to Numeric
```{r}
df[, 1:25] <- sapply(df[, 1:25], as.numeric)
str(df)
```

# Explore Credit Card Default Dataset
```{r}
dim(df)
str(df)
summary(df)
```

# Find Categories outside of Data Dictionary w/ EDUCATION & MARRIAGE
```{r}
# Update EDUCATION Category 0,5,6 to Category 4 (others) 
# (1 = graduate school; 2 = university; 3 = high school; 4 = others)
count(df, vars = EDUCATION)

df$EDUCATION[df$EDUCATION == 0] <- 4
df$EDUCATION[df$EDUCATION == 5] <- 4
df$EDUCATION[df$EDUCATION == 6] <- 4

# Update MARRIAGE Category 0 to Category 3 (others) 
# (1 = married; 2 = single; 3 = others)
count(df, vars = MARRIAGE)

df$MARRIAGE[df$MARRIAGE == 0] <- 3

```

# Plot Correlation w/ Reference to Target Var "default_payment"
```{r}

corrplot(cor(df)
         , method = 'color'      # Visualization method for the correlation matrix
         , order = 'hclust'      # Orders the variables so that ones that behave similarly are placed next to each other
         , addCoef.col = 'black' # Color of coefficients on the graph
         , number.cex = .3       # Lower values decrease the size of the numbers in the cells
         )
```

Can observe from Correlation Plot the Weak Correlation of Age, 
BILL_AMT1,BILL_AMT2,BILL_AMT3,BILL_AMT4,BILL_AMT5,BILL_AMT6
to the Target Variable 'Default Payment'

# Delete Columns with Low Correlation Values
```{r}
df_new <- select(df,-ID,-BILL_AMT1,-BILL_AMT2,-BILL_AMT3,-BILL_AMT4,-BILL_AMT5,-BILL_AMT6)
head(df_new)
```

# Scale first 18 columns in df_new
```{r}
df_new[,1:17] <- scale(df_new[, 1:17])
head(df_new)
```

# Partition Data into Train and Test (70% Train / 30% Test)
```{r}
contrasts(factor(df_new$default_payment))
library(caret)
set.seed(77) 
partition <- caret::createDataPartition(y=df_new$default_payment, p=.70, list=FALSE) 
data_train <- df_new[partition,]
data_test <- df_new[-partition,]

print(nrow(data_train)/(nrow(data_test)+nrow(data_train)))
dim(data_train)
dim(data_test)
```

# Train the Model
```{r}
model_train <- glm(default_payment ~ ., family=binomial, data=data_train)
summary(model_train)
```

We can see from the model that some Categories are not statistically significant
These include PAY_2, PAY_4, PAY_5, PAY_6, PAY_AMT4 which have p val > 0.05.
The most statistically significant categories are PAY_0 and Intercept (default_payment)
Along with LIMIT_BAL, PAY_AMT1, PAY_AMT2

# Prediction and Evaluate Model for Training Data
```{r}
predict_train <- predict(model_train, newdata=data_train, type='response')
print(summary(predict_train))
data_train$prediction <- predict_train
head(data_train, n=10)
```

# Confusion Matrix for Training Data
```{r}
table1 <- table(predict_train>0.5, data_train$default_payment) #prediction on left and truth on top
my_confusion_matrix(table1)
```

# Predict and Evaluate Model on Test Data
```{r}
predict_test <- predict(model_train, newdata=data_test, type='response')
print(summary(predict_test))
data_test$prediction <- predict_test
head(data_test, n=10)
table2 <- table(predict_test>.5, data_test$default_payment) #prediction on left and truth on top
my_confusion_matrix(table2)
```

# Writeup Summary
Write a summary that includes the following: business problem (point 5 above), description and justification of the ML algorithm chosen, comments on the model performance, and insights from the model results. The suggested length is between 250 and 500 words

# Business Problem:
Objective is to predict the probability of a Credit Card Client Defaulting on Payments.
Target Variable is default_payments and independent variables include 
Limit Bal, Sex, Education Level, Marriage, Age, Pay and Pay Amount. 

# Description and justification of ML Algorithm Used
Utilized the algorithm Logistic Regression to predict the probability of default payments 
And split the data into Training and Testing Data Sets.
I used Logistic Regression due to its ease of use, easy to interpret and efficient to train.

# Training Data Model Performance
Training Data performance resulted in 
High Accuracy (81.27%) which shows model predicting the probability of default payments correctly.
Low sensitivity (25%) How many positives model got right
High Specificity (97%) How many negative model got right
High Precision (72%) How good models positive predictions
High Negative Predictive Value (82%) How good models negative predictions
Overall Model did well but not as good at predicting positives correctly.

# Testing Data Model Performance
Testing Data performance resulted in 
High Accuracy (80.8%) which shows model predicting the probability of default payments correctly.
Low sensitivity (23%) How many positives model got right
High Specificity (97.4%) How many negative model got right
High Precision (71.89%) How good models positive predictions
High Negative Predictive Value (81.6%) How good models negative predictions
Overall Model did well but not as good at predicting positives correctly like training data.

# Insight from Logistic Regression Algorithm
Through this model we can see which months have lower coefficient values from the summary of the model and learn which months are best for explaining the default payment target variable.
Through this scenario, months PAY_2 & PAY_4 have low coefficient values which are the months of August and June while higher values are PAY_0 & PAY_3 which are months of September and July.
In addition the low coefficients from PAY_AMT1 & PAY_AMT2 which are Sept and Aug
And high coefficients PAY_AMT4 & PAY_AMT5 which are June and May.

This gives the credit lenders an idea of which months to expect Credit Card Default Payments
as well as figure out the trend for the best time of year to lend credit and expecting to be paid back from the Credit Card users.

