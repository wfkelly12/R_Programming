# J073 HW7 Group Assignment - KNN / Decision Trees
Course: MBA563
Term: Summer 2021
Module: 07
Eric Boyle, Quaidzoher Dhilla, Brandon Hyer, Will Kelly, Christian Mukania, and Vinh Tran

# KNN & Decision Trees

# Initial loading of data, packages, and functions
```{r}
# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
  true_positive <- cf_table[4]
  true_negative <- cf_table[1]
  false_positive <- cf_table[2]
  false_negative <- cf_table[3]
  accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
  sensitivity_recall <- true_positive / (true_positive + false_negative) 
  specificity_selectivity <- true_negative / (true_negative + false_positive)
  precision <- true_positive / (true_positive + false_positive) 
  neg_pred_value <- true_negative/(true_negative + false_negative)
  print(cf_table)
  my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
                  sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
                  sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
                  sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
                  sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy), 
                  sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
                  sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
                  sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
                  sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
  )
  return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds("mod6HE_logit.rds")
```


# Preprocess data for knn
```{r}
# Not for the model
knn1 <- df %>% ungroup() %>% 
  select(store, week, region, high_med_rev, high_med_gp, high_med_gpm)

# Make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high'))) 
knn2 <- knn2 %>% ungroup() %>% 
  select(high_med_units, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)

# Check that "positive" is last for the `my_confusion_matrix` to work 
contrasts(knn2$high_med_units)
```


# Partition the data
```{r}
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_units, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]

# Separate the target variable from the training and testing data 
X_train <- data_train %>% select(-high_med_units)
X_test <-  data_test %>% select(-high_med_units) 
y_train <- data_train$high_med_units
y_test <- data_test$high_med_units
```


# Features must be standardized so use z-score standardization
```{r}
X_train <- scale(X_train)
X_test <- scale(X_test)
```


# Run the model
```{r}
#install.packages('class') #don't install twice
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
```

*Go look at questions in PowerPoint.*

# Confusion matrix - checking accuracy
```{r}
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
```


# Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$knn <- knn_prediction

# Create a variable that shows if the prediction was correct
data_test <- data_test %>% 
  mutate(correct_knn = if_else(knn == high_med_units, 'correct', 'WRONG!'))

# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)

# For viewing in class
full_test_knn <- full_test_knn %>% 
  select(store, week, high_med_units, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
```


# Preprocess data Decision Trees
```{r}
# Not for the model
tree1 <- df %>% ungroup() %>% 
  select(store, week, high_med_rev, high_med_gp, high_med_gpm)

# make the target feature and `region` a factor
tree2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')), region = factor(region)) 
tree2 <- tree2 %>% ungroup() %>% 
  select(high_med_units, size, region, promo_units_per, 
         altbev_units_per, confect_units_per, salty_units_per, 
         velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)

# Check that "positive" is last for `my_confusion_matrix()` to work 
contrasts(tree2$high_med_units)
```


# Use the `caret` package to split the data, 75% training and 25% testing
```{r}
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_units, p=.75, list=FALSE)
data_train <- tree2[partition, ]
data_test <- tree2[-partition, ]
```


# Use the `rpart()` function from the `rpart` package to train the model
```{r}
#install.packages('rpart') #(don't install twice)
#install.packages('rpart.plot') #(don't install twice)
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_units ~ ., data_train)
```


# Use the trained model to predict whether `high_med_units` is high or low
```{r}
predict_tree <- predict(model_tree, data_test, type='class') #`type='class'` keeps this a factor 
```


# Use the confusion matrix code above to examine the accuracy of this model
```{r}
table1 <- table(predict_tree, data_test$high_med_units)
my_confusion_matrix(table1)
```


# Using the `plot()` function draw a labeled picture of the tree model.
```{r}
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'gray', nn=TRUE)
```


# Put the data back together for future use
```{r}
# Put the prediction back into the test data
data_test$tree <- predict_tree

# Create a variable that shows if the prediction was correct
data_test <- data_test %>% 
  mutate(correct_tree = if_else(tree == high_med_units, 'correct', 'WRONG!'))

# Add back the original data
temp1 <- tree1[-partition, ]
full_test_tree <- bind_cols(temp1, data_test)

# For viewing in class
full_test_tree <- full_test_tree %>% 
  select(store, week, high_med_units, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)
```


# Put both predictions together
```{r}
full_test <- bind_cols(full_test_knn %>% select(store, week, high_med_units, knn, correct_knn), 
                       full_test_tree %>% select(-store, -week, -high_med_units))
slice_sample(full_test, n=10)
```

# Questions
# 1. Is type 1 or type 2 error higher for the KNN model? That is, which has a higher number?

•	The Type 1 Error (324) is higher for the KNN model vs Type 2 (215) 

# 2. Which aspect of the accuracy of the KNN model is better—sensitivity (hit rate) or specificity (true negative rate)?
a. Write your answer here.

•	In this model the Sensitivity (hit rate) (83%) is greater than Specificity (74%)
    
b. Explain the above answer. That is, which individual components of these measures (either true positive, true negative, false positive, or false negative) led to sensitivity (hit rate) or specificity (true negative rate) being better than the other?

•	The True Positive (1042) led to the Sensitivity (hit rate) to be higher than specificity.
In this case the True Negative was (936). 
    
c. What does this mean about the business that this model is examining? Write less than three sentences.

•	This indicates that the business has a higher probability of above median units sold compared to below the median due to the higher hit rate (sensitivity) when using high_med_units as target variable. 

d. The nine measures of accuracy provided in the ` my_confusion_matrix()` function output are not the only measures of accuracy. Do an internet search and describe two other measures of accuracy. These measures may or may not be derived from these nine measures.

•	Positive Likelihood Ratio = Sensitivity / (1 - Specificity)
•	Negative likelihood Ratio = (1 - Sensitivity) / Specificity

# 3. Interpret the decision tree output by answering the following questions:

a. Start at the beginning of the tree, the root node. What is the most important factor for above median units sold? That is, if you had to pick one feature that told you the most, what would it be? Said even another way, which feature keeps popping up in the tree?

•	The feature 'Size' is present the most in this Decision Tree.

b. Using the decision tree, if you are a smaller store (a store that offers less than 980 products for sale), which regions are more likely to lead to above median units sold?

•	The regions Ontario & Atlantic.  

# 4. Look back to your group assignment for Module 6 in which you were asked to use logistic regression to examine median units sold (`high_med_units`). Over the course of that assignment and this current assignment you have used three algorithms to examine this feature variable (`high_med_units`). Suppose you were working on an engagement to help NANSE better predict and understand when a store in a particular week will sell an above average number of units. In particular, suppose NANSE engaged you to answer the questions below. Respond to each question by picking which of the three algorithms is the most helpful in answering the question and explain why by comparing and contrasting to the other models. Even though your client is unlikely to care about this, for pedagogical purposes, make sure to discuss what it is about the model that you pick that makes it more helpful than the others. Your response to a. and b. should be about three sentences each.

a. “We would like to build a company-wide dashboard next year that tells us at the end of each week which stores sold enough units to be in the top half of units sold for that year, even though the year is not over. Can you use the data from the year that just ended to create a predictive model that, with a high degree of accuracy, tells us which of our stores in a given week is likely to sell above median units?”

•	Model: Linear Regression, Logistic Regression, KNN, Decision Trees
TBD~
Logistic Regression or Knn:


b. “In addition to this dashboard, we would like to use last year’s data to understand which variables help our stores have successful weeks. Can you use that data to tell us which factors are most important at helping our stores have above median units sold in a given week?”

•	Model: Linear Regression, Logisitic Regression, KNN, Decision Trees
TBD~
Decision Tree:
The Decision Tree would help discover which factors have the most significant impact through its Flow chart-like iterative decision process giving the analysts greater visibility.
The algorithm selects the attribute with highest information gain and splits data on basis of attribute. 
The features that occur the most will point the analysts to the most important factors that are important in helping stores have above median units sold in a given week.
Advantages of the Decision tree are its intuitive, easily interpreted and ability to handle qualitative predictors without creating dummy variables.


# 5. Our discussions on Coursera and in class have focused on three classification algorithms (logistic regression, k-nearest neighbors, and decision trees). Many other classification algorithms exist and new ones are being developed all of the time. As you continue to expand your skills you will need to develop the ability to use the framework we are providing in this class to learn new algorithms. Do some research and find a classification algorithm that we have not discussed (not one of these three).
a. List this algorithm.

•	Neural Network Algorithm (NN)

b. List several advantages and disadvantages of this algorithm. If possible, compare and contrast it to the three algorithms we studied.

•	Advantages
Flexible and can be used with both regression and classification problems.
Good for the nonlinear dataset with large number of inputs such as images.
Neural Networks can work with any number of inputs and layers.

•	Disadvantages
NNs much more of a black box method and requires more time for development and computational power.
NNs require more data than other Machine Learning Algorithms.
NNs can be used only with numerical inputs and non-missing value datasets.

c. Find two relevant lines of R code that are used to run this algorithm and paste them below.

•	#install.packages(neuralnet)

library(neuralnet)

set.seed(7896129)

nnmodel <- neuralnet(f,data=bnk_matrix,hidden=1,
                    threshold = 0.01,
                    learningrate.limit = NULL,
                    learningrate.factor =
                         list(minus = 0.5, plus = 1.2),
                    algorithm = "rprop+")
                    
                    
                  


