sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy),
sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
)
return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds("mod6HE_logit.rds")
# Not for the model
knn1 <- df %>% ungroup() %>%
select(store, week, region, high_med_rev, high_med_units, high_med_gpm)
# Make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_gp = factor(if_else(high_med_gp==1, 'high', 'low'), levels=c('low', 'high')))
knn2 <- knn2 %>% ungroup() %>%
select(high_med_gp, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)
# Check that "positive" is last for the `my_confusion_matrix` to work
contrasts(knn2$high_med_gp)
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_gp, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]
# Separate the target variable from the training and testing data
X_train <- data_train %>% select(-high_med_gp)
X_test <-  data_test %>% select(-high_med_gp)
y_train <- data_train$high_med_gp
y_test <- data_test$high_med_gp
X_train <- scale(X_train)
X_test <- scale(X_test)
#install.packages('class') #don't install twice
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
# Put the prediction back into the test data
data_test$knn <- knn_prediction
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_knn = if_else(knn == high_med_gp, 'correct', 'WRONG!'))
# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)
# For viewing in class
full_test_knn <- full_test_knn %>%
select(store, week, high_med_gp, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
# Not for the model
tree1 <- df %>% ungroup() %>%
select(store, week, high_med_rev, high_med_units, high_med_gpm)
# make the target feature and `region` a factor
tree2 <- df %>% mutate(high_med_gp = factor(if_else(high_med_gp==1, 'high', 'low'), levels=c('low', 'high')),
region = factor(region))
tree2 <- tree2 %>% ungroup() %>%
select(high_med_gp, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Check that "positive" is last for `my_confusion_matrix()` to work
contrasts(tree2$high_med_gp)
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_gp, p=.75, list=FALSE)
data_train <- tree2[partition, ]
data_test <- tree2[-partition, ]
#install.packages('rpart') #(don't install twice)
#install.packages('rpart.plot') #(don't install twice)
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_gp ~ ., data_train)
predict_tree <- predict(model_tree, data_test, type='class') #`type='class'` keeps this a factor
table1 <- table(predict_tree, data_test$high_med_gp)
my_confusion_matrix(table1)
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'gray', nn=TRUE)
# Put the prediction back into the test data
data_test$tree <- predict_tree
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_tree = if_else(tree == high_med_gp, 'correct', 'WRONG!'))
# Add back the original data
temp1 <- tree1[-partition, ]
full_test_tree <- bind_cols(temp1, data_test)
# For viewing in class
full_test_tree <- full_test_tree %>%
select(store, week, high_med_gp, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)
full_test <- bind_cols(full_test_knn %>% select(store, week, high_med_gp, knn, correct_knn),
full_test_tree %>% select(-store, -week, -high_med_gp))
slice_sample(full_test, n=10)
View(df)
# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
true_positive <- cf_table[4]
true_negative <- cf_table[1]
false_positive <- cf_table[2]
false_negative <- cf_table[3]
accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
sensitivity_recall <- true_positive / (true_positive + false_negative)
specificity_selectivity <- true_negative / (true_negative + false_positive)
precision <- true_positive / (true_positive + false_positive)
neg_pred_value <- true_negative/(true_negative + false_negative)
print(cf_table)
my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy),
sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
)
return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds("mod6HE_logit.rds")
# Not for the model
knn1 <- df %>% ungroup() %>%
select(store, week, region, high_med_rev, high_med_gp, high_med_gpm)
# Make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')))
knn2 <- knn2 %>% ungroup() %>%
select(high_med_units, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)
# Check that "positive" is last for the `my_confusion_matrix` to work
contrasts(knn2$high_med_units)
View(knn2)
View(knn2)
View(knn1)
View(knn2)
View(df)
# Not for the model
knn1 <- df %>% ungroup() %>%
select(store, week, region, high_med_rev, high_med_gp, high_med_gpm)
# Make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')))
knn2 <- knn2 %>% ungroup() %>%
select(high_med_units, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)
# Check that "positive" is last for the `my_confusion_matrix` to work
contrasts(knn2$high_med_units)
View(knn2)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_units, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]
# Separate the target variable from the training and testing data
X_train <- data_train %>% select(-high_med_units)
X_test <-  data_test %>% select(-high_med_units)
y_train <- data_train$high_med_units
y_test <- data_test$high_med_units
View(data_train)
X_train <- scale(X_train)
X_test <- scale(X_test)
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
# Put the prediction back into the test data
data_test$knn <- knn_prediction
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_knn = if_else(knn == high_med_units, 'correct', 'WRONG!'))
# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)
# For viewing in class
full_test_knn <- full_test_knn %>%
select(store, week, high_med_units, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
true_positive <- cf_table[4]
true_negative <- cf_table[1]
false_positive <- cf_table[2]
false_negative <- cf_table[3]
accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
sensitivity_recall <- true_positive / (true_positive + false_negative)
specificity_selectivity <- true_negative / (true_negative + false_positive)
precision <- true_positive / (true_positive + false_positive)
neg_pred_value <- true_negative/(true_negative + false_negative)
print(cf_table)
my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy),
sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
)
return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds("mod6HE_logit.rds")
# Not for the model
knn1 <- df %>% ungroup() %>%
select(store, week, region, high_med_rev, high_med_gp, high_med_gpm)
# Make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')))
knn2 <- knn2 %>% ungroup() %>%
select(high_med_units, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)
# Check that "positive" is last for the `my_confusion_matrix` to work
contrasts(knn2$high_med_units)
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_units, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]
# Separate the target variable from the training and testing data
X_train <- data_train %>% select(-high_med_units)
X_test <-  data_test %>% select(-high_med_units)
y_train <- data_train$high_med_units
y_test <- data_test$high_med_units
X_train <- scale(X_train)
X_test <- scale(X_test)
#install.packages('class') #don't install twice
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
# Put the prediction back into the test data
data_test$knn <- knn_prediction
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_knn = if_else(knn == high_med_units, 'correct', 'WRONG!'))
# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)
# For viewing in class
full_test_knn <- full_test_knn %>%
select(store, week, high_med_units, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
# Not for the model
tree1 <- df %>% ungroup() %>%
select(store, week, high_med_rev, high_med_gp, high_med_gpm)
# make the target feature and `region` a factor
tree2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')), region = factor(region))
tree2 <- tree2 %>% ungroup() %>%
select(high_med_units, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Check that "positive" is last for `my_confusion_matrix()` to work
contrasts(tree2$high_med_units)
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_units, p=.75, list=FALSE)
data_train <- tree2[partition, ]
data_test <- tree2[-partition, ]
#install.packages('rpart') #(don't install twice)
#install.packages('rpart.plot') #(don't install twice)
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_units ~ ., data_train)
predict_tree <- predict(model_tree, data_test, type='class') #`type='class'` keeps this a factor
table1 <- table(predict_tree, data_test$high_med_units)
my_confusion_matrix(table1)
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'gray', nn=TRUE)
# Put the prediction back into the test data
data_test$tree <- predict_tree
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_tree = if_else(tree == high_med_units, 'correct', 'WRONG!'))
# Add back the original data
temp1 <- tree1[-partition, ]
full_test_tree <- bind_cols(temp1, data_test)
# For viewing in class
full_test_tree <- full_test_tree %>%
select(store, week, high_med_units, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)
full_test <- bind_cols(full_test_knn %>% select(store, week, high_med_units, knn, correct_knn),
full_test_tree %>% select(-store, -week, -high_med_units))
slice_sample(full_test, n=10)
# Run this reusable confusion matrix function (https://en.wikipedia.org/wiki/Confusion_matrix)
my_confusion_matrix <- function(cf_table) {
true_positive <- cf_table[4]
true_negative <- cf_table[1]
false_positive <- cf_table[2]
false_negative <- cf_table[3]
accuracy <- (true_positive + true_negative) / (true_positive + true_negative + false_positive + false_negative)
sensitivity_recall <- true_positive / (true_positive + false_negative)
specificity_selectivity <- true_negative / (true_negative + false_positive)
precision <- true_positive / (true_positive + false_positive)
neg_pred_value <- true_negative/(true_negative + false_negative)
print(cf_table)
my_list <- list(sprintf("%1.0f = True Positive (TP), Hit", true_positive),
sprintf("%1.0f = True Negative (TN), Rejection", true_negative),
sprintf("%1.0f = False Positive (FP), Type 1 Error", false_positive),
sprintf("%1.0f = False Negative (FN), Type 2 Error", false_negative),
sprintf("%1.4f = Accuracy (TP+TN/(TP+TN+FP+FN))", accuracy),
sprintf("%1.4f = Sensitivity, Recall, Hit Rate, True Positive Rate (How many positives did the model get right? TP/(TP+FN))", sensitivity_recall),
sprintf("%1.4f = Specificity, Selectivity, True Negative Rate (How many negatives did the model get right? TN/(TN+FP))", specificity_selectivity),
sprintf("%1.4f = Precision, Positive Predictive Value (How good are the model's positive predictions? TP/(TP+FP))", precision),
sprintf("%1.4f = Negative Predictive Value (How good are the model's negative predictions? TN/(TN+FN)", neg_pred_value)
)
return(my_list)
}
# Install and load packages (don't install twice)
#install.packages('tidyverse')
library(tidyverse)
# Load data
df <- read_rds("mod6HE_logit.rds")
# Not for the model
knn1 <- df %>% ungroup() %>%
select(store, week, region, high_med_rev, high_med_gp, high_med_gpm)
# Make the target feature a factor and put the "low" level first so `my_confusion_matrix()` works correctly
knn2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')))
knn2 <- knn2 %>% ungroup() %>%
select(high_med_units, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Data must be numeric so one-hot encode `region`
#install.packages('fastDummies') #(don't install twice)
library(fastDummies)
knn2 <- fastDummies::dummy_cols(knn2, select_columns = c("region"), remove_selected_columns=T)
# Check that "positive" is last for the `my_confusion_matrix` to work
contrasts(knn2$high_med_units)
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=knn2$high_med_units, p=.75, list=FALSE)
data_train <- knn2[partition, ]
data_test <- knn2[-partition, ]
# Separate the target variable from the training and testing data
X_train <- data_train %>% select(-high_med_units)
X_test <-  data_test %>% select(-high_med_units)
y_train <- data_train$high_med_units
y_test <- data_test$high_med_units
X_train <- scale(X_train)
X_test <- scale(X_test)
#install.packages('class') #don't install twice
library(class)
knn_prediction = class::knn(train=X_train, test=X_test, cl=y_train, k=round(sqrt(nrow(data_train))/2))
table2 <- table(knn_prediction, y_test) #prediction on left and truth on top
my_confusion_matrix(table2)
# Put the prediction back into the test data
data_test$knn <- knn_prediction
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_knn = if_else(knn == high_med_units, 'correct', 'WRONG!'))
# Add back the original data to the test data
temp1 <- knn1[-partition, ]
full_test_knn <- bind_cols(temp1, data_test)
# For viewing in class
full_test_knn <- full_test_knn %>%
select(store, week, high_med_units, knn, correct_knn, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_knn, n=10)
# Not for the model
tree1 <- df %>% ungroup() %>%
select(store, week, high_med_rev, high_med_gp, high_med_gpm)
# make the target feature and `region` a factor
tree2 <- df %>% mutate(high_med_units = factor(if_else(high_med_units==1, 'high', 'low'), levels=c('low', 'high')), region = factor(region))
tree2 <- tree2 %>% ungroup() %>%
select(high_med_units, size, region, promo_units_per,
altbev_units_per, confect_units_per, salty_units_per,
velocityA_units_per, velocityB_units_per, velocityC_units_per, velocityD_units_per, velocityNEW_units_per)
# Check that "positive" is last for `my_confusion_matrix()` to work
contrasts(tree2$high_med_units)
#install.packages('caret') #(don't install twice)
library(caret)
set.seed(77)
partition <- caret::createDataPartition(y=tree2$high_med_units, p=.75, list=FALSE)
data_train <- tree2[partition, ]
data_test <- tree2[-partition, ]
#install.packages('rpart') #(don't install twice)
#install.packages('rpart.plot') #(don't install twice)
library(rpart)
library(rpart.plot)
model_tree <- rpart::rpart(high_med_units ~ ., data_train)
predict_tree <- predict(model_tree, data_test, type='class') #`type='class'` keeps this a factor
table1 <- table(predict_tree, data_test$high_med_units)
my_confusion_matrix(table1)
rpart.plot::rpart.plot(model_tree, box.palette = 'RdBu', shadow.col = 'gray', nn=TRUE)
# Put the prediction back into the test data
data_test$tree <- predict_tree
# Create a variable that shows if the prediction was correct
data_test <- data_test %>%
mutate(correct_tree = if_else(tree == high_med_units, 'correct', 'WRONG!'))
# Add back the original data
temp1 <- tree1[-partition, ]
full_test_tree <- bind_cols(temp1, data_test)
# For viewing in class
full_test_tree <- full_test_tree %>%
select(store, week, high_med_units, tree, correct_tree, size, region, promo_units_per, salty_units_per)
slice_sample(full_test_tree, n=10)
full_test <- bind_cols(full_test_knn %>% select(store, week, high_med_units, knn, correct_knn),
full_test_tree %>% select(-store, -week, -high_med_units))
slice_sample(full_test, n=10)
setwd("~/Documents/iMBA Classes/MBA 563/MOOC2 Machine Learning Algorithms w R in BA/Module 4/Module 4 Coursera")
library(tidyverse)
#install.packages('factoextra')
library(factoextra)
options(tibble.print_max = Inf)
options(tibble.width = Inf)
#install.packages('factoextra')
library(factoextra)
install.packages('factoextra')
library(tidyverse)
# install.packages('factoextra')
library(factoextra)
options(tibble.print_max = Inf)
options(tibble.width = Inf)
setwd("~/Documents/iMBA Classes/MBA 563/MOOC2 Machine Learning Algorithms w R in BA/Module 4/Module 4 Coursera")
clustering_input2 <- read_rds('clustering_input2.rds')
clustering_input2 <- read_rds("clustering_input2.rds")
clustering_input2 <- read_rds('clustering_input2.rds')
library(tidyverse)
clustering_input2 <- read_rds('clustering_input2.rds')
setwd("~/Documents/iMBA Classes/MBA 563/MOOC2 Machine Learning Algorithms w R in BA/Module 4/Mod 4 Coursera")
setwd("~/Documents/iMBA Classes/MBA 563/MOOC2 Machine Learning Algorithms w R in BA/Module 4/Module 4 Coursera")
library(tidyverse)
#install.packages('factoextra')
library(factoextra)
options(tibble.print_max = Inf)
options(tibble.width = Inf)
clustering_input2 <- read_rds('clustering_input2.rds')
str(clustering_input2)
slice_sample(clustering_input2, n=10)
clustering_input2 <- read_rds('clustering_input2.rds')
str(clustering_input2)
slice_sample(clustering_input2, n=10)
clustering_input2 <- as.data.frame(scale(clustering_input2))
View(clustering_input2)
set.seed(777)
clusters <- kmeans(clustering_input2, centers=7, iter.max=20, nstart=1)
clusters$size
clusters$centers
clustering_input1 <- read_rds('clustering_input1.rds')
clustering_input1$cluster <- clusters$cluster
clustering_input1 <- clustering_input1 %>% mutate(cluster_labels = case_when(
cluster==1 ~ 'everything',
cluster==2 ~ 'fountain soda/coffee',
cluster==3 ~ 'gas only',
cluster==4 ~ 'cigarettes',
cluster==5 ~ 'meals',
cluster==6 ~ 'snacks',
cluster==7 ~ 'cooler'))
clustering_input1[53:55, ]
View(clustering_input1)
View(clusters)
View(clusters)
clusters$cluster
fviz_cluster(clusters, clustering_input2,  geom = "point", show.clust.cent = FALSE, palette = "jco", ggtheme = theme_classic())
library(tidyverse)
#install.packages('dbscan')
library(dbscan)
library(factoextra)
options(tibble.print_max = Inf)
options(tibble.width = Inf)
clustering_input2 <- read_rds('clustering_input2.rds')
str(clustering_input2)
slice_sample(clustering_input2, n=10)
View(clustering_input2)
clustering_input2 <- as.data.frame(scale(clustering_input2))
View(clustering_input2)
set.seed(777)
clustering_input2_s <- clustering_input2[sample(nrow(clustering_input2), 15000), ]
dbscan::kNNdistplot(clustering_input2_s, k = 7)
abline(h = 4)
dbscan::kNNdistplot(clustering_input2_s, k = 7)
abline(h = 4)
set.seed(777)
clusters_db <- dbscan(clustering_input2_s, eps = 4, minPts = 7)
View(clusters_db)
table(clusters_db$cluster)
fviz_cluster(clusters_db, clustering_input2_s,  geom = "point", show.clust.cent = FALSE, palette = "jco", ggtheme = theme_classic())
fviz_cluster(clusters_db, clustering_input2_s,  geom = "point", show.clust.cent = FALSE, palette = "jco", ggtheme = theme_classic())
clustering_input2_s$cluster <- clusters_db$cluster
clustering_look <- clustering_input2_s %>% group_by(cluster) %>% summarize(n=n(), across('area.fresh':'refill.yes', mean))
clustering_look
View(clustering_input2_s)
View(clustering_look)
setwd("~/Documents/iMBA Classes/MBA 563/MOOC2 Machine Learning Algorithms w R in BA/Module 8 Mini Assignment")
df <- read.csv("default of credit card clients.csv")
setwd("~/Documents/iMBA Classes/MBA 563/MOOC2 Machine Learning Algorithms w R in BA/Module 8 Mini Assignment")
